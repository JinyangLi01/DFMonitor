{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7250465470ae36b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# explore the dataset",
   "id": "f8729bc1cb5aa943"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T20:49:32.766734Z",
     "start_time": "2024-09-17T20:49:30.163572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import pandas as pd\n",
    "from river import datasets\n",
    "import numpy as np\n",
    "\n",
    "dataset = datasets.MovieLens100K()\n",
    "# Initialize last item variable\n",
    "last_item = None\n",
    "# Inspect the first few examples\n",
    "first_item = None\n",
    "for i, (x, y) in enumerate(dataset):\n",
    "    if i == 0:\n",
    "        first_item = (x, y)\n",
    "    # print(f\"Example {i+1}\")\n",
    "    # print(\"Features:\", x)\n",
    "    datetime = pd.to_datetime(x[\"timestamp\"], unit='ns')\n",
    "    # print(datetime)\n",
    "    # print(\"Rating:\", y)\n",
    "    # print()  # Blank line for readability\n",
    "    last_item = (x, y)\n",
    "\n",
    "\n"
   ],
   "id": "ea0fab2a51856fc7",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T20:51:03.591759Z",
     "start_time": "2024-09-17T20:51:03.589209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "datetime_first = pd.to_datetime(first_item[0][\"timestamp\"], unit='ns')\n",
    "datetime_last = pd.to_datetime(last_item[0][\"timestamp\"], unit='ns')\n",
    "print(\"first item datetime = \", datetime_first, \"last item datetime = \", datetime_last)"
   ],
   "id": "df002632b8e8c47e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first item datetime =  1997-09-20 05:05:10 last item datetime =  1998-04-23 01:10:38\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T20:51:34.546681Z",
     "start_time": "2024-09-17T20:51:34.542511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# how many days apart\n",
    "datetime_last - datetime_first"
   ],
   "id": "26f02bb3c1ad5401",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timedelta('214 days 20:05:28')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "59376c25c25884bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T21:48:22.981751Z",
     "start_time": "2024-09-17T21:48:20.661649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "from river import datasets\n",
    "\n",
    "# Initialize data structures\n",
    "user_ratings = defaultdict(dict)\n",
    "\n",
    "# Load the MovieLens 100K dataset\n",
    "dataset = datasets.MovieLens100K()\n",
    "\n",
    "# Build user_ratings structure from MovieLens data\n",
    "def load_movielens_data():\n",
    "    for x, y in dataset:\n",
    "        user_id = x['user']   # Extract the user ID\n",
    "        item_id = x['item']   # Extract the item ID\n",
    "        rating = y            # Extract the rating value\n",
    "        user_ratings[user_id][item_id] = rating  # Store the rating in user_ratings\n",
    "\n",
    "# Create user pairs and count common items\n",
    "def compute_common_items_distribution(user_ratings):\n",
    "    common_item_distribution = defaultdict(int)  # Key: number of common items, Value: frequency\n",
    "    \n",
    "    # Get all unique pairs of users\n",
    "    user_pairs = combinations(user_ratings.keys(), 2)\n",
    "    \n",
    "    for user1, user2 in user_pairs:\n",
    "        # Find common items between two users\n",
    "        common_items = set(user_ratings[user1]) & set(user_ratings[user2])\n",
    "        \n",
    "        # Count the number of common items\n",
    "        common_item_count = len(common_items)\n",
    "        \n",
    "        # Update the distribution\n",
    "        common_item_distribution[common_item_count] += 1\n",
    "    \n",
    "    return common_item_distribution\n",
    "\n",
    "# Function to print the distribution in a readable format\n",
    "def print_common_item_distribution(common_item_distribution):\n",
    "    print(f\"{'Common Items':<15} {'User Pair Count':<15}\")\n",
    "    print(\"-\" * 30)\n",
    "    for common_items, count in sorted(common_item_distribution.items()):\n",
    "        print(f\"{common_items:<15} {count:<15}\")\n",
    "\n",
    "# Load the data\n",
    "load_movielens_data()\n",
    "\n",
    "# Run the common item computation\n",
    "common_item_distribution = compute_common_items_distribution(user_ratings)\n",
    "\n",
    "# Print the result\n",
    "print_common_item_distribution(common_item_distribution)\n"
   ],
   "id": "66739dbcb9793a2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Items    User Pair Count\n",
      "------------------------------\n",
      "0               15043          \n",
      "1               22359          \n",
      "2               26088          \n",
      "3               27027          \n",
      "4               25725          \n",
      "5               24279          \n",
      "6               22423          \n",
      "7               20617          \n",
      "8               18598          \n",
      "9               16941          \n",
      "10              15307          \n",
      "11              14062          \n",
      "12              12483          \n",
      "13              11413          \n",
      "14              10029          \n",
      "15              9148           \n",
      "16              8215           \n",
      "17              7668           \n",
      "18              6753           \n",
      "19              6410           \n",
      "20              5800           \n",
      "21              5370           \n",
      "22              4925           \n",
      "23              4716           \n",
      "24              4309           \n",
      "25              4069           \n",
      "26              3766           \n",
      "27              3491           \n",
      "28              3296           \n",
      "29              3059           \n",
      "30              2973           \n",
      "31              2874           \n",
      "32              2764           \n",
      "33              2575           \n",
      "34              2372           \n",
      "35              2277           \n",
      "36              2227           \n",
      "37              2113           \n",
      "38              1930           \n",
      "39              1838           \n",
      "40              1814           \n",
      "41              1768           \n",
      "42              1720           \n",
      "43              1562           \n",
      "44              1450           \n",
      "45              1438           \n",
      "46              1443           \n",
      "47              1379           \n",
      "48              1311           \n",
      "49              1227           \n",
      "50              1247           \n",
      "51              1227           \n",
      "52              1171           \n",
      "53              1093           \n",
      "54              1075           \n",
      "55              958            \n",
      "56              971            \n",
      "57              997            \n",
      "58              887            \n",
      "59              872            \n",
      "60              920            \n",
      "61              870            \n",
      "62              846            \n",
      "63              798            \n",
      "64              780            \n",
      "65              795            \n",
      "66              697            \n",
      "67              728            \n",
      "68              693            \n",
      "69              666            \n",
      "70              678            \n",
      "71              679            \n",
      "72              616            \n",
      "73              567            \n",
      "74              596            \n",
      "75              535            \n",
      "76              565            \n",
      "77              547            \n",
      "78              524            \n",
      "79              567            \n",
      "80              524            \n",
      "81              484            \n",
      "82              467            \n",
      "83              467            \n",
      "84              466            \n",
      "85              417            \n",
      "86              407            \n",
      "87              372            \n",
      "88              395            \n",
      "89              376            \n",
      "90              369            \n",
      "91              376            \n",
      "92              381            \n",
      "93              345            \n",
      "94              361            \n",
      "95              333            \n",
      "96              338            \n",
      "97              277            \n",
      "98              300            \n",
      "99              299            \n",
      "100             279            \n",
      "101             268            \n",
      "102             275            \n",
      "103             266            \n",
      "104             244            \n",
      "105             227            \n",
      "106             250            \n",
      "107             216            \n",
      "108             244            \n",
      "109             215            \n",
      "110             199            \n",
      "111             214            \n",
      "112             170            \n",
      "113             187            \n",
      "114             188            \n",
      "115             195            \n",
      "116             182            \n",
      "117             185            \n",
      "118             145            \n",
      "119             149            \n",
      "120             140            \n",
      "121             144            \n",
      "122             135            \n",
      "123             153            \n",
      "124             154            \n",
      "125             135            \n",
      "126             144            \n",
      "127             137            \n",
      "128             132            \n",
      "129             116            \n",
      "130             121            \n",
      "131             110            \n",
      "132             133            \n",
      "133             110            \n",
      "134             107            \n",
      "135             114            \n",
      "136             106            \n",
      "137             75             \n",
      "138             94             \n",
      "139             103            \n",
      "140             82             \n",
      "141             90             \n",
      "142             77             \n",
      "143             65             \n",
      "144             88             \n",
      "145             71             \n",
      "146             65             \n",
      "147             79             \n",
      "148             58             \n",
      "149             69             \n",
      "150             56             \n",
      "151             70             \n",
      "152             73             \n",
      "153             60             \n",
      "154             56             \n",
      "155             60             \n",
      "156             61             \n",
      "157             61             \n",
      "158             52             \n",
      "159             52             \n",
      "160             53             \n",
      "161             44             \n",
      "162             51             \n",
      "163             60             \n",
      "164             50             \n",
      "165             42             \n",
      "166             45             \n",
      "167             35             \n",
      "168             35             \n",
      "169             30             \n",
      "170             32             \n",
      "171             30             \n",
      "172             26             \n",
      "173             49             \n",
      "174             26             \n",
      "175             27             \n",
      "176             27             \n",
      "177             35             \n",
      "178             24             \n",
      "179             39             \n",
      "180             16             \n",
      "181             27             \n",
      "182             29             \n",
      "183             30             \n",
      "184             23             \n",
      "185             29             \n",
      "186             22             \n",
      "187             24             \n",
      "188             29             \n",
      "189             14             \n",
      "190             31             \n",
      "191             18             \n",
      "192             14             \n",
      "193             18             \n",
      "194             8              \n",
      "195             11             \n",
      "196             18             \n",
      "197             15             \n",
      "198             18             \n",
      "199             15             \n",
      "200             16             \n",
      "201             18             \n",
      "202             17             \n",
      "203             18             \n",
      "204             12             \n",
      "205             15             \n",
      "206             19             \n",
      "207             9              \n",
      "208             11             \n",
      "209             15             \n",
      "210             14             \n",
      "211             12             \n",
      "212             14             \n",
      "213             10             \n",
      "214             7              \n",
      "215             8              \n",
      "216             9              \n",
      "217             5              \n",
      "218             6              \n",
      "219             9              \n",
      "220             4              \n",
      "221             8              \n",
      "222             11             \n",
      "223             9              \n",
      "224             8              \n",
      "225             14             \n",
      "226             5              \n",
      "227             6              \n",
      "228             5              \n",
      "229             4              \n",
      "230             3              \n",
      "231             6              \n",
      "232             12             \n",
      "233             6              \n",
      "234             9              \n",
      "235             5              \n",
      "236             8              \n",
      "237             6              \n",
      "238             1              \n",
      "239             3              \n",
      "240             2              \n",
      "241             4              \n",
      "242             5              \n",
      "243             7              \n",
      "244             5              \n",
      "245             3              \n",
      "246             1              \n",
      "247             2              \n",
      "248             7              \n",
      "249             3              \n",
      "250             4              \n",
      "251             2              \n",
      "252             2              \n",
      "253             3              \n",
      "254             3              \n",
      "255             4              \n",
      "256             4              \n",
      "257             7              \n",
      "258             3              \n",
      "259             3              \n",
      "260             2              \n",
      "261             2              \n",
      "262             2              \n",
      "263             4              \n",
      "264             3              \n",
      "265             2              \n",
      "266             2              \n",
      "267             1              \n",
      "268             3              \n",
      "269             2              \n",
      "271             1              \n",
      "273             1              \n",
      "274             1              \n",
      "276             1              \n",
      "277             1              \n",
      "278             2              \n",
      "279             2              \n",
      "281             2              \n",
      "282             2              \n",
      "283             1              \n",
      "285             2              \n",
      "287             2              \n",
      "288             1              \n",
      "290             1              \n",
      "292             2              \n",
      "293             1              \n",
      "294             1              \n",
      "299             1              \n",
      "300             1              \n",
      "301             1              \n",
      "303             1              \n",
      "305             1              \n",
      "311             1              \n",
      "314             1              \n",
      "315             1              \n",
      "318             1              \n",
      "319             1              \n",
      "327             2              \n",
      "332             2              \n",
      "334             1              \n",
      "335             1              \n",
      "346             1              \n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8d24c53a53537ad7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# collaborative_filtering with exponential time decay (GPT)",
   "id": "40087b909979ef88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T21:53:51.634662Z",
     "start_time": "2024-09-17T21:53:51.620587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from river import datasets, metrics\n",
    "\n",
    "# Initialize data structures\n",
    "user_ratings = defaultdict(dict)\n",
    "user_timestamps = defaultdict(dict)\n",
    "similarity = defaultdict(dict)\n",
    "\n",
    "# Initialize classification metrics\n",
    "accuracy = metrics.Accuracy()\n",
    "precision = metrics.Precision()\n",
    "recall = metrics.Recall()\n",
    "f1 = metrics.F1()\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.MovieLens100K()\n",
    "\n",
    "headers = [\"user\", \"item\", \"timestamp\", \"title\", \"release_date\", \"genres\",\n",
    "           \"age\", \"age_2groups\", \"age_4groups\", \"gender\", \"occupation\", \"zip_code\",\n",
    "           \"rating\", \"prediction\", \"datetime\", \"rating_binary\", \"prediction_binary\",\n",
    "           \"diff\", \"diff_binary_correctness\"]\n",
    "\n",
    "# Function to convert nanoseconds timestamp to days\n",
    "def convert_to_days(nanosecond_timestamp):\n",
    "    return nanosecond_timestamp / (86400 * 1e9)\n",
    "\n",
    "def time_decay_weight(new_time_in_days, last_time_in_days, alpha):\n",
    "    \"\"\"\n",
    "    Calculate the time decay weight for an interaction.\n",
    "\n",
    "    Parameters:\n",
    "    - new_time_in_days: The current interaction timestamp in days.\n",
    "    - last_time_in_days: The previous interaction timestamp in days.\n",
    "    - alpha: The decay rate.\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the time decay weight.\n",
    "    \"\"\"\n",
    "    time_difference = new_time_in_days - last_time_in_days\n",
    "    return alpha ** time_difference\n",
    "\n",
    "def compute_similarity(user1, user2, current_time_in_days, decay_rate):\n",
    "    \"\"\"\n",
    "    Compute the similarity between two users using time-decayed ratings.\n",
    "\n",
    "    Parameters:\n",
    "    - user1, user2: The user IDs.\n",
    "    - current_time_in_days: The current timestamp in days.\n",
    "    - decay_rate: The time decay rate.\n",
    "\n",
    "    Returns:\n",
    "    - A float representing the similarity between user1 and user2.\n",
    "    \"\"\"\n",
    "    common_items = set(user_ratings[user1]) & set(user_ratings[user2])\n",
    "    # print(\"user1 = \", user1, \"user2 = \", user2, \"common items = \", common_items)\n",
    "    if not common_items:\n",
    "        return 0\n",
    "    if len(common_items) == 1:\n",
    "        return 0\n",
    "    print(\"common items more than 1\")\n",
    "    print(common_items)\n",
    "    ratings1 = []\n",
    "    ratings2 = []\n",
    "    for item in common_items:\n",
    "        # Calculate time decay weights for both users\n",
    "        time1 = convert_to_days(user_timestamps[user1][item])\n",
    "        time2 = convert_to_days(user_timestamps[user2][item])\n",
    "        weight1 = time_decay_weight(time1, current_time_in_days, decay_rate)\n",
    "        weight2 = time_decay_weight(time2, current_time_in_days, decay_rate)\n",
    "        ratings1.append(user_ratings[user1][item] * weight1)\n",
    "        ratings2.append(user_ratings[user2][item] * weight2)\n",
    "    print(\"ratings1 = \", ratings1)\n",
    "    print(\"ratings2 = \", ratings2)\n",
    "    # Use Pearson correlation coefficient\n",
    "    mean1 = sum(ratings1) / len(ratings1)\n",
    "    mean2 = sum(ratings2) / len(ratings2)\n",
    "    numerator = sum((r1 - mean1) * (r2 - mean2) for r1, r2 in zip(ratings1, ratings2))\n",
    "    denominator = (sum((r - mean1) ** 2 for r in ratings1) * sum((r - mean2) ** 2 for r in ratings2)) ** 0.5\n",
    "    print(\"numerator = \", numerator, \"denominator = \", denominator)\n",
    "    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "\n",
    "def run(result_file_name, decay_rate, accuracy, precision, recall, f1, dataset, headers):\n",
    "    print(\"run with alpha = \", decay_rate, \"result file name = \", result_file_name)\n",
    "    \n",
    "   # Convert dataset to a list to sort by timestamp\n",
    "    dataset_list = list(dataset)\n",
    "    \n",
    "    # Sort the dataset by timestamp\n",
    "    dataset_list.sort(key=lambda x: x[0]['timestamp'])\n",
    "    \n",
    "    # Initialize the clock with the earliest timestamp in days\n",
    "    last_time_in_days = convert_to_days(dataset_list[0][0]['timestamp'])\n",
    "\n",
    "\n",
    "   # Open the CSV file for writing and write the header\n",
    "    with open(result_file_name, \"w\", newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for x, y in dataset_list:\n",
    "            user_id = x['user']\n",
    "            item_id = x['item']\n",
    "\n",
    "            # Convert the current interaction timestamp to days\n",
    "            new_time_in_days = convert_to_days(x['timestamp'])\n",
    "\n",
    "            # Calculate the time decay weight between the last and the current interaction\n",
    "            decay_weight = time_decay_weight(new_time_in_days, last_time_in_days, alpha)\n",
    "\n",
    "            # Update the last_time_in_days to the current interaction time\n",
    "            last_time_in_days = new_time_in_days\n",
    "\n",
    "            # Initialize prediction\n",
    "            pred = None\n",
    "\n",
    "            # Only proceed if user_id is already in user_ratings\n",
    "            if user_id in user_ratings:\n",
    "                similarities = {}\n",
    "                for other_user in user_ratings:\n",
    "                    if other_user != user_id:\n",
    "                        # Check if similarity has been computed before\n",
    "                        if user_id in similarity and other_user in similarity[user_id]:\n",
    "                            sim = similarity[user_id][other_user]\n",
    "                            # print(\"sim = \", sim)\n",
    "                        else:\n",
    "                            sim = compute_similarity(user_id, other_user, new_time_in_days, alpha)\n",
    "                            # Store the computed similarity\n",
    "                            similarity[user_id][other_user] = sim\n",
    "                            similarity[other_user][user_id] = sim\n",
    "                            # print(\"sim = \", sim)\n",
    "                        # Only consider users with positive similarity who have rated the item\n",
    "                        if sim > 0 and item_id in user_ratings[other_user]:\n",
    "                            similarities[other_user] = sim\n",
    "\n",
    "                if similarities:\n",
    "                    numerator = 0.0\n",
    "                    denominator = 0.0\n",
    "                    for other_user in similarities:\n",
    "                        # Apply time decay to the neighbor's rating\n",
    "                        time_of_rating_in_days = convert_to_days(user_timestamps[other_user][item_id])\n",
    "                        weight = time_decay_weight(time_of_rating_in_days, new_time_in_days, alpha)\n",
    "                        numerator += similarities[other_user] * user_ratings[other_user][item_id] * weight\n",
    "                        denominator += abs(similarities[other_user]) * weight\n",
    "                    pred = numerator / denominator if denominator != 0 else None\n",
    "\n",
    "            # Default prediction if None\n",
    "            if pred is None:\n",
    "                pred = 3.0  # Average rating in MovieLens dataset\n",
    "\n",
    "            # Add the actual rating and the prediction to the dictionary\n",
    "            x['rating'] = y\n",
    "            x['prediction'] = pred\n",
    "            preds = pred\n",
    "\n",
    "            x[\"rating_binary\"] = int(y >= 4)\n",
    "            x[\"prediction_binary\"] = int(preds >= 4) if preds is not None else None\n",
    "            x[\"diff\"] = abs(y - preds)\n",
    "            x[\"diff_binary_correctness\"] = int(abs(y - preds) <= 1)\n",
    "            x[\"datetime\"] = pd.to_datetime(x['timestamp'], unit='ns').strftime('%Y-%m-%d')\n",
    "\n",
    "            # Age groupings\n",
    "            if x[\"age\"] <= 30:\n",
    "                x[\"age_2groups\"] = \"7-30\"\n",
    "            else:\n",
    "                x[\"age_2groups\"] = \"31-73\"\n",
    "\n",
    "            if x[\"age\"] <= 24:\n",
    "                x[\"age_4groups\"] = \"7-24\"\n",
    "            elif x[\"age\"] <= 30:\n",
    "                x[\"age_4groups\"] = \"25-30\"\n",
    "            elif x[\"age\"] <= 40:\n",
    "                x[\"age_4groups\"] = \"31-40\"\n",
    "            else:\n",
    "                x[\"age_4groups\"] = \"41-73\"\n",
    "\n",
    "            writer.writerow(x)\n",
    "\n",
    "            # Update the user ratings and timestamps AFTER making the prediction\n",
    "            user_ratings[user_id][item_id] = y\n",
    "            user_timestamps[user_id][item_id] = x['timestamp']\n",
    "\n",
    "            # Update classification metrics\n",
    "            if preds is not None:\n",
    "                y_binary = int(y >= 4)\n",
    "                pred_binary = int(preds >= 4)\n",
    "                accuracy.update(y_binary, pred_binary)\n",
    "                precision.update(y_binary, pred_binary)\n",
    "                recall.update(y_binary, pred_binary)\n",
    "                f1.update(y_binary, pred_binary)\n",
    "\n",
    "    # Output the final results\n",
    "    print(f\"Accuracy: {accuracy.get():.4f}\")\n",
    "    print(f\"Precision: {precision.get():.4f}\")\n",
    "    print(f\"Recall: {recall.get():.4f}\")\n",
    "    print(f\"F1 Score: {f1.get():.4f}\")\n"
   ],
   "id": "6b01a093dc15818f",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T21:54:03.931304Z",
     "start_time": "2024-09-17T21:53:52.718969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "# find the best alpha value\n",
    "for alpha in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]:\n",
    "    accuracy = metrics.Accuracy()\n",
    "    precision = metrics.Precision()\n",
    "    recall = metrics.Recall()\n",
    "    f1 = metrics.F1()\n",
    "     # Load the dataset\n",
    "    dataset = datasets.MovieLens100K()\n",
    "\n",
    "    headers = [\"user\", \"item\", \"timestamp\", \"title\", \"release_date\", \"genres\",\n",
    "               \"age\", \"age_2groups\", \"age_4groups\", \"gender\", \"occupation\", \"zip_code\",\n",
    "               \"rating\", \"prediction\", \"datetime\", \"rating_binary\", \"prediction_binary\",\n",
    "               \"diff\", \"diff_binary_correctness\"]\n",
    "    \n",
    "    result_file = \"movielens_online_cf_time_decay_alpha_10e\" + str(int(math.log10(alpha))) + \".csv\"\n",
    "    decay_rate = alpha  # Adjust this value as needed\n",
    "    run(result_file, decay_rate, accuracy, precision, recall, f1, dataset, headers)\n",
    "    break\n",
    "    "
   ],
   "id": "45e36165e234f5b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run with alpha =  0.1 result file name =  movielens_online_cf_time_decay_alpha_10e-1.csv\n",
      "Accuracy: 0.4462\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n",
   "id": "7b1f51f15b34d5df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "504958cb058c45c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "20f15a10fda3b996"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "52636867e1fa4878"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e0dfd42b31fdbfb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# collaborative_filtering with exponential time decay (github)  FAIL\n",
    "https://github.com/gucino/Temporal-Collaborative-Filtering-using-decay-function-to-track-dynamic-interest-of-user/blob/master/Temporal_CF_decay_funnction.py"
   ],
   "id": "5a9b78c11ec4721b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T23:15:57.825248Z",
     "start_time": "2024-09-16T23:15:57.705789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun May 10 21:20:54 2020\n",
    "\n",
    "@author: Tisana\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from river import datasets\n",
    "\n",
    "# Load the dataset\n",
    "data_set = datasets.MovieLens100K()\n",
    "\n",
    "\n",
    "##clean data\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "#compute user rating matrix and timestamp matrix\n",
    "num_user=943 #user id 1 to 943\n",
    "num_movie=1682 #movie id 1 to 1682\n",
    "\n",
    "user_rating_dict={}\n",
    "#key is user id : value are rating of all movie \n",
    "for user_id in range(1,num_user+1):\n",
    "    user_rating_dict[user_id]=np.array([0]*num_movie)\n",
    "    \n",
    "user_timestamp_dict={}\n",
    "for user_id in range(1,num_user+1):\n",
    "    user_timestamp_dict[user_id]=np.array([0]*num_movie)\n",
    "\n",
    "#append rating data set to user rating dict\n",
    "data_set_list=data_set.tolist()\n",
    "for each_row in data_set_list:\n",
    "    user_id=each_row[0]\n",
    "    mmovie_id=each_row[1]\n",
    "    rating=each_row[2]\n",
    "    movie_index=mmovie_id-1\n",
    "    timestamp=each_row[3]\n",
    "    #append to dictionary\n",
    "    user_rating_dict[user_id][movie_index]=rating\n",
    "    user_timestamp_dict[user_id][movie_index]=timestamp\n",
    "\n",
    "user_rating_array=[]\n",
    "for each in user_rating_dict.values():\n",
    "    user_rating_array.append(each)\n",
    "user_rating_array=np.array(user_rating_array) #index by user index (user id -1)\n",
    "\n",
    "#convert rating matrix to user-like matrix\n",
    "user_like_matrix=[]\n",
    "for i in range(0,num_user):\n",
    "    row_list=[]\n",
    "    for j in range(0,num_movie):\n",
    "        rating=user_rating_array[i,j]\n",
    "        if rating>=3:\n",
    "            row_list.append(1)\n",
    "        else:\n",
    "            row_list.append(0)\n",
    "    user_like_matrix.append(np.array(row_list))\n",
    "user_like_matrix=np.array(user_like_matrix)\n",
    "\n",
    "#convert user-like matrix to user-user network\n",
    "user_user_network=[]\n",
    "for i in range(0,num_user):\n",
    "    if i%10==0:\n",
    "        print(i)\n",
    "    row_list=[]\n",
    "    for j in range(0,num_user):\n",
    "        common_prefered_item=user_like_matrix[i,:]*user_like_matrix[j,:]\n",
    "        row_list.append(common_prefered_item)\n",
    "    row_list=np.array(row_list).sum(axis=1)\n",
    "    user_user_network.append(row_list)\n",
    "user_user_network=np.array(user_user_network)\n",
    "#normalization\n",
    "row_mean=np.mean(user_rating_array,axis=1)\n",
    "row_mean=row_mean[:,np.newaxis]\n",
    "user_rating_array=(user_rating_array-row_mean)*(user_rating_array)/(user_rating_array)\n",
    "\n",
    "for each_row in range(0,num_user):\n",
    "    for each_column in range(0,num_movie):\n",
    "        if np.isnan(user_rating_array[each_row,each_column])==True:\n",
    "            user_rating_array[each_row,each_column]=0\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "#get timestamp matrix\n",
    "user_timestamp_array=[]\n",
    "for each in user_timestamp_dict.values():\n",
    "    user_timestamp_array.append(each)\n",
    "user_timestamp_array=np.array(user_timestamp_array) \n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################\n",
    "#compute user similarity matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "user_similarity_matrix=[]\n",
    "for i in range(0,num_user):\n",
    "    if i%10==0:\n",
    "        print(i,\" out of \",num_user)\n",
    "    user_1_id=i+1\n",
    "    row=[]\n",
    "    for j in range(0,num_user):\n",
    "        user_2_id=j+1\n",
    "        similarity=pearsonr(user_rating_array[user_1_id-1],user_rating_array[user_2_id-1])[0]\n",
    "        #similarity=cosine_similarity([user_rating_array[user_1_id-1]],[user_rating_array[user_2_id-1]])[0][0]\n",
    "        row.append(similarity)\n",
    "    user_similarity_matrix.append(np.array(row))    \n",
    "user_similarity_matrix=np.array(user_similarity_matrix)    \n",
    "\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################   \n",
    "#prediction function\n",
    "\n",
    "def faster_rating_prediction(k,user_similarity_matrix,time,alpha,user_timestamp_array):\n",
    "    #avg rating of all user matrix\n",
    "    avg_rating_user_matrix=np.mean(user_rating_array,axis=1)\n",
    "    avg_rating_user_matrix=avg_rating_user_matrix[:,np.newaxis]\n",
    "    avg_rating_user_matrix=np.repeat(avg_rating_user_matrix,num_movie,axis=1)\n",
    "    \n",
    "    predicted_rating_array=[]\n",
    "    for target_user_index in range(0,num_user):\n",
    "        \n",
    "        #avg rating of target user\n",
    "        avg_rating_of_target_user=avg_rating_user_matrix[target_user_index,:]\n",
    "        \n",
    "        #find k similar user\n",
    "        lst=pd.Series(list(user_similarity_matrix[target_user_index,:]))\n",
    "        i=lst.nlargest(k+1)\n",
    "        similar_user_index_list=i.index.values.tolist()\n",
    "        similar_user_index_list=similar_user_index_list[1:] #exclude yourself\n",
    "        \n",
    "        #avg rating of similar user\n",
    "        avg_rating_of_similar_user=avg_rating_user_matrix[similar_user_index_list,:]\n",
    "        rating_of_similar_user=user_rating_array[similar_user_index_list,:]\n",
    "        diff_of_similar_user=rating_of_similar_user-avg_rating_of_similar_user\n",
    "        \n",
    "        \n",
    "        \n",
    "        time_diff=weighted_time(target_user_index,similar_user_index_list,alpha,user_timestamp_array)\n",
    "        \n",
    "        #check for time\n",
    "        if time==True:\n",
    "            diff_of_similar_user=diff_of_similar_user*time_diff\n",
    "        \n",
    "        #second term\n",
    "        similarity_to_target_user=user_similarity_matrix[target_user_index,similar_user_index_list]\n",
    "        similarity_to_target_user=similarity_to_target_user[:,np.newaxis]\n",
    "        numerator=sum(diff_of_similar_user*similarity_to_target_user)\n",
    "        \n",
    "        if time==True:\n",
    "            denominator=sum(similarity_to_target_user*time_diff)\n",
    "        else:\n",
    "            denominator=sum(similarity_to_target_user)\n",
    "        \n",
    "        \n",
    "        second_term=numerator/denominator\n",
    "        \n",
    "        #prediction\n",
    "        predicted_rating_of_target_user=avg_rating_of_target_user+second_term\n",
    "        predicted_rating_array.append(predicted_rating_of_target_user)\n",
    "\n",
    "    predicted_rating_array=np.array(predicted_rating_array)\n",
    "    return predicted_rating_array\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "######################################################################## \n",
    "#MAE function\n",
    "def MAE_calculator(predicted_user_rating_array,user_rating_array):\n",
    "    #change predict matrix to have only known value\n",
    "    filter_matrix=np.copy(user_rating_array)\n",
    "    filter_matrix[filter_matrix>0]=1\n",
    "    predicted_user_rating_array=predicted_user_rating_array*filter_matrix\n",
    "    \n",
    "    num_predict=np.count_nonzero(predicted_user_rating_array)\n",
    "    MAE=(abs(predicted_user_rating_array-user_rating_array).sum())/num_predict\n",
    "    return MAE\n",
    "\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "########################################################################     \n",
    "\n",
    "#generate abs time diff matrix\n",
    "def weighted_time(target_user_index,similar_user_index_list,alpha,user_timestamp_array):\n",
    "\n",
    "\n",
    "    a=user_timestamp_array[target_user_index,:]\n",
    "    b=user_timestamp_array[similar_user_index_list,:]\n",
    "    time_diff_matrix=abs(a-b)\n",
    "    \n",
    "    #standardization\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler=StandardScaler()\n",
    "    time_diff_matrix = scaler.fit_transform(time_diff_matrix)\n",
    "    lam_matrix=np.exp(-1*time_diff_matrix*alpha)\n",
    "    return lam_matrix\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "######################################################################## \n",
    "#find best value of alpha (1.7)\n",
    "k=3\n",
    "MAE_list=[]\n",
    "alpha_list=[]\n",
    "alpha=0\n",
    "for i in range(0,100):\n",
    "    predicted_rating=faster_rating_prediction(k,user_similarity_matrix,True,alpha,user_timestamp_array)\n",
    "    MAE=MAE_calculator(predicted_rating,user_rating_array)\n",
    "    MAE_list.append(MAE)\n",
    "    alpha_list.append(alpha)\n",
    "    print(\" MAE : \",MAE)\n",
    "    alpha+=0.1\n",
    "plt.title(\"find best value of alpha\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.plot(alpha_list,MAE_list)\n",
    "best_alpha_index=MAE_list.index(min(MAE_list))\n",
    "best_alpha=alpha_list[best_alpha_index]\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "######################################################################## \n",
    "#compare performance of no time and time\n",
    "alpha=best_alpha\n",
    "MAE_time_list=[]\n",
    "MAE_no_time_list=[]\n",
    "k_list=[]\n",
    "for k in range(1,100,10):\n",
    "    print(\"k : \",k)\n",
    "    time=faster_rating_prediction(k,user_similarity_matrix,True,alpha,user_timestamp_array)\n",
    "    no_time=faster_rating_prediction(k,user_similarity_matrix,False,alpha,user_timestamp_array)\n",
    "    \n",
    "    MAE_time=MAE_calculator(time,user_rating_array)\n",
    "    MAE_no_time=MAE_calculator(no_time,user_rating_array)\n",
    "    \n",
    "    MAE_time_list.append(MAE_time)\n",
    "    MAE_no_time_list.append(MAE_no_time)\n",
    "    k_list.append(k)\n",
    "plt.figure()\n",
    "plt.xlabel(\"number of neighbourhood\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.plot(k_list,MAE_time_list,c=\"green\",label=\"consider dynamic user interest\")\n",
    "plt.plot(k_list,MAE_no_time_list,c=\"red\",label=\"do not consider dynamic user interest\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "b743e9b8113644f7",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MovieLens100K' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 35\u001B[0m\n\u001B[1;32m     32\u001B[0m     user_timestamp_dict[user_id]\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m*\u001B[39mnum_movie)\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m#append rating data set to user rating dict\u001B[39;00m\n\u001B[0;32m---> 35\u001B[0m data_set_list\u001B[38;5;241m=\u001B[39m\u001B[43mdata_set\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtolist\u001B[49m()\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m each_row \u001B[38;5;129;01min\u001B[39;00m data_set_list:\n\u001B[1;32m     37\u001B[0m     user_id\u001B[38;5;241m=\u001B[39meach_row[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'MovieLens100K' object has no attribute 'tolist'"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T23:02:06.935109Z",
     "start_time": "2024-09-16T23:01:56.140154Z"
    }
   },
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt",
   "id": "3b59696fa87df16e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T23:02:06.943378Z",
     "start_time": "2024-09-16T23:02:06.942030Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "61c28ac28358aa27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "317989b425479ff1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
